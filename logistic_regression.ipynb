{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    '''\n",
    "    Function to calculate sigmoid function of z\n",
    "    '''\n",
    "    return 1.0/(1+np.exp(-z))\n",
    "\n",
    "\n",
    "def predict_probability(X, theta):\n",
    "    '''\n",
    "    Function to calculate probability of being positive class\n",
    "    '''\n",
    "    z = np.matmul(X, theta)\n",
    "    return sigmoid(z)\n",
    "\n",
    "\n",
    "def predict_class(probability):\n",
    "    '''\n",
    "    Function to classify as positive if probability greater or equal than threshold of 0.5\n",
    "    '''\n",
    "    prediction = np.where(probability>=0.5, 1, 0)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def loss_function(predictions, labels):\n",
    "    '''\n",
    "    Log loss function\n",
    "    '''\n",
    "    loss = -labels * np.log(predictions) - (1-labels)*np.log(1-predictions)\n",
    "    return np.mean(loss)\n",
    "\n",
    "\n",
    "def init_params(dims):\n",
    "    '''\n",
    "    Function to initialise the weight parameters to random numbers and the momentum to 0s\n",
    "    '''\n",
    "    theta = np.random.standard_normal(dims)\n",
    "    v_t = np.zeros_like(theta)\n",
    "    return theta, v_t\n",
    "\n",
    "\n",
    "def plotLosses(train_losses, val_losses):\n",
    "    '''\n",
    "    Function to plot the training and validation losses acquired during training\n",
    "    '''\n",
    "    plt.figure(figsize=(10,5))\n",
    "    epochs = np.arange(0,len(train_losses))\n",
    "    plt.plot(np.arange(0,len(train_losses)),train_losses, color='b', label='Training')\n",
    "    plt.plot(np.arange(0,len(val_losses)),val_losses, color='r', label='Validation')\n",
    "    plt.xticks(np.arange(0,len(train_losses), step=round(len(train_losses)/10)))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LogReg(X_train, y_train, X_val, y_val, max_epochs=200, lr=0.01, gamma=0.9):\n",
    "    '''\n",
    "    Function to train a logisitc regression model\n",
    "    '''\n",
    "    #initialise weights to random numbers, and velocity to all 0s\n",
    "    theta, v_t = init_params(X_train.shape[1])\n",
    "\n",
    "    #track model parameters\n",
    "    model_dict = {\n",
    "        'best_loss' : np.inf,\n",
    "        'best_epoch' : 0,\n",
    "        'best_weights' : 1,\n",
    "        'train_losses' : [],\n",
    "        'val_losses' : []\n",
    "    }\n",
    "\n",
    "    #train and val losses\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    #begin training\n",
    "    tik = time.time()\n",
    "    for epoch in range(0,max_epochs):\n",
    "\n",
    "        #training set probability predictions and loss calculation\n",
    "        probs = predict_probability(X_train, theta)\n",
    "        train_loss = loss_function(probs, y_train)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        #calculate gradient\n",
    "        grad = np.matmul(X_train.T,(probs-y_train)) / len(X_train)\n",
    "\n",
    "        #momentum\n",
    "        v_t = gamma*v_t + lr*grad\n",
    "\n",
    "        #update weights\n",
    "        #theta -= lr*grad\n",
    "        theta -= v_t\n",
    "\n",
    "        #validation set probability predictions and loss calculation\n",
    "        val_probs = predict_probability(X_val, theta)\n",
    "        val_loss = loss_function(val_probs, y_val)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        \n",
    "        #update best weights when validation loss improves\n",
    "        if val_loss < model_dict['best_loss']:\n",
    "            model_dict['best_loss'] = val_loss\n",
    "            model_dict['best_epoch'] = epoch\n",
    "            model_dict['best_weights'] = theta\n",
    "            model_dict['train_losses'] = train_losses\n",
    "            model_dict['val_losses'] = val_losses\n",
    "        \n",
    "        if epoch%round(max_epochs/10)==0:\n",
    "            print(\"[Epoch: {}] \\t Training loss: {:.6f} \\t Validation loss: {:.6f}\".format(epoch, train_loss, val_loss))\n",
    "\n",
    "        #convergence criteria kicks in after epochs above 5\n",
    "        if epoch>20:\n",
    "            #criteria (1): validation loss less than it was 10 epochs ago\n",
    "            if val_losses[-1] > val_losses[-11]:\n",
    "                print('Convergence criteria (1) met: Increase in validation loss, as compared to 11 updates prior')\n",
    "                return model_dict\n",
    "            #criteria (2): validation loss decreased less than 0.01% from last epoch\n",
    "            if ((val_losses[-2] - val_losses[-1]) / val_losses[-2]) < 0.0001:\n",
    "                print('Convergence criteria (2) met: Decrease in validation loss less than 0.01%')\n",
    "                return model_dict\n",
    "    \n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_LogReg(X_train, y_train, X_val, y_val, max_epochs=200, lr=0.01, gamma=0.9)\n",
    "plotLosses(model['train_losses'], model['val_losses'])"
   ]
  }
 ]
}